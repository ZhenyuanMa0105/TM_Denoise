{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:23.417148Z",
     "iopub.status.busy": "2024-09-04T22:03:23.416794Z",
     "iopub.status.idle": "2024-09-04T22:03:25.308513Z",
     "shell.execute_reply": "2024-09-04T22:03:25.307984Z",
     "shell.execute_reply.started": "2024-09-04T22:03:23.417116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print('done')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:27.833022Z",
     "iopub.status.busy": "2024-09-04T22:03:27.832639Z",
     "iopub.status.idle": "2024-09-04T22:03:30.305669Z",
     "shell.execute_reply": "2024-09-04T22:03:30.305209Z",
     "shell.execute_reply.started": "2024-09-04T22:03:27.833002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check GPU availability\n",
    "use_gpu = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_gpu \\\n",
    "        else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:30.306786Z",
     "iopub.status.busy": "2024-09-04T22:03:30.306494Z",
     "iopub.status.idle": "2024-09-04T22:03:30.322597Z",
     "shell.execute_reply": "2024-09-04T22:03:30.322203Z",
     "shell.execute_reply.started": "2024-09-04T22:03:30.306771Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small function to plot images\n",
    "def showTensor(aTensor, pos=111):\n",
    "    plt.subplot(int(pos))\n",
    "    plt.imshow(aTensor)    #,cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    #plt.show()\n",
    "    plt.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:30.429585Z",
     "iopub.status.busy": "2024-09-04T22:03:30.429270Z",
     "iopub.status.idle": "2024-09-04T22:03:30.446583Z",
     "shell.execute_reply": "2024-09-04T22:03:30.446189Z",
     "shell.execute_reply.started": "2024-09-04T22:03:30.429570Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to create complex tensors (ie, split real & img parts) + transfer between numpy and pytorch\n",
    "\n",
    "def to_complex_tensor(stuff):\n",
    "    # TO_COMPLEX_TENSOR transforms a complex nparray into a complex tensor\n",
    "    # The real and imaginary part are concatenated along the last dimension\n",
    "    \n",
    "    # Retrieve real and imaginary parts in separate tensors\n",
    "    stuff_r = torch.from_numpy(np.real(stuff)).to(device).float()\n",
    "    stuff_i = torch.from_numpy(np.imag(stuff)).to(device).float()\n",
    "    \n",
    "    # Add a last dimension\n",
    "    stuff_r = torch.unsqueeze(stuff_r, dim=stuff_r.ndimension())\n",
    "    stuff_i = torch.unsqueeze(stuff_i, dim=stuff_i.ndimension())\n",
    "    \n",
    "    # Return the concatenation\n",
    "    return torch.cat((stuff_r, stuff_i), dim=-1)\n",
    "\n",
    "def from_complex_tensor(stuff):\n",
    "    # FROM_COMPLEX_TENSOR transforms a complex tensor into a complex nparray\n",
    "    \n",
    "    # Retrieve real and imaginary parts in separate tensors\n",
    "    (stuff_r, stuff_i) = torch.chunk(stuff, 2, dim=-1)\n",
    "    # Convert to numpy\n",
    "    stuff_r = np.squeeze(stuff_r.cpu().detach().numpy())\n",
    "    stuff_i = np.squeeze(stuff_i.cpu().detach().numpy())\n",
    "    # Return the complex-valued nparray\n",
    "    return stuff_r + 1j * stuff_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:32.637601Z",
     "iopub.status.busy": "2024-09-04T22:03:32.637307Z",
     "iopub.status.idle": "2024-09-04T22:03:32.658311Z",
     "shell.execute_reply": "2024-09-04T22:03:32.657856Z",
     "shell.execute_reply.started": "2024-09-04T22:03:32.637586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to compute complex tensors (eg: tensor muliplication) which are not implemented in Pythorch\n",
    "\n",
    "def complex_matmul(a, b):\n",
    "    # COMPLEX_MATMUL computes a@b with a and b complex on the specified device\n",
    "\n",
    "    # Ensure that inputs are on the correct device and are in the expected format\n",
    "    a = a.to(device)\n",
    "    b = b.to(device)\n",
    "\n",
    "    # Retrieve real and imaginary parts in separate tensors\n",
    "    (a_r, a_i) = torch.chunk(a, 2, dim=-1)\n",
    "    (b_r, b_i) = torch.chunk(b, 2, dim=-1)\n",
    "    \n",
    "    # Remove the last dimension\n",
    "    a_r = a_r.squeeze(-1)\n",
    "    a_i = a_i.squeeze(-1)\n",
    "    b_r = b_r.squeeze(-1)\n",
    "    b_i = b_i.squeeze(-1)\n",
    "    \n",
    "    # Compute the multiplication\n",
    "    real_part = a_r @ b_r - a_i @ b_i\n",
    "    imag_part = a_i @ b_r + a_r @ b_i\n",
    "    \n",
    "    # Return the concatenation, re-adding the last dimension\n",
    "    result = torch.cat((real_part.unsqueeze(-1), imag_part.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "#     return torch.cat((real_part[:, np.newaxis], imag_part[:, np.newaxis]), dim=-1)\n",
    "\n",
    "def complex_multiply(a, b):\n",
    "    # COMPLEX_MULTIPLY computes a*b with a and b complex\n",
    "\n",
    "    # Retrieve real and imaginary parts in separate tensors\n",
    "    (a_r, a_i) = torch.chunk(a, 2, dim=-1)\n",
    "    (b_r, b_i) = torch.chunk(b, 2, dim=-1)\n",
    "    # Remove the last dimension\n",
    "    a_r = torch.squeeze(a_r)\n",
    "    a_i = torch.squeeze(a_i)\n",
    "    b_r = torch.squeeze(b_r)\n",
    "    b_i = torch.squeeze(b_i)\n",
    "    # Compute the multiplication\n",
    "    real_part = a_r * b_r - a_i * b_i\n",
    "    imag_part = a_i * b_r + a_r * b_i\n",
    "    # Return the concatenation\n",
    "    return torch.cat((real_part.unsqueeze_(-1), imag_part.unsqueeze_(-1)), dim=-1)\n",
    "\n",
    "def complex_norm2(b):\n",
    "    # COMPLEX_NORM2 computes the L2 norm square of b\n",
    "    return torch.sum(b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:35.157875Z",
     "iopub.status.busy": "2024-09-04T22:03:35.157579Z",
     "iopub.status.idle": "2024-09-04T22:03:35.174904Z",
     "shell.execute_reply": "2024-09-04T22:03:35.174471Z",
     "shell.execute_reply.started": "2024-09-04T22:03:35.157860Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phase_to_complex(phi):\n",
    "    # PHASE_TO_COMPLEX transforms a real-valued tensor into a complex tensor\n",
    "    # Compute the real and imaginary parts in separate tensors    \n",
    "    stuff_r = torch.cos(phi)\n",
    "    stuff_i = torch.sin(phi)\n",
    "    # Add a last dimension\n",
    "    stuff_r = torch.unsqueeze(stuff_r, dim=stuff_r.ndimension())\n",
    "    stuff_i = torch.unsqueeze(stuff_i, dim=stuff_i.ndimension())\n",
    "    # Return the concatenation\n",
    "    return torch.cat((stuff_r, stuff_i), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incoherent Process (eg: Fluorescence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:49.810886Z",
     "iopub.status.busy": "2024-09-04T22:03:49.810577Z",
     "iopub.status.idle": "2024-09-04T22:03:50.103681Z",
     "shell.execute_reply": "2024-09-04T22:03:50.103193Z",
     "shell.execute_reply.started": "2024-09-04T22:03:49.810867Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "def resize_image(image, new_size):\n",
    "    # scale factor\n",
    "    scale = [n / float(o) for n, o in zip(new_size, image.shape)]\n",
    "    return zoom(image, zoom=scale, order=1)\n",
    "\n",
    "train_num = 100\n",
    "val_num = 20\n",
    "\n",
    "train_images = read_idx('MNIST/train/train-images.idx3-ubyte')\n",
    "\n",
    "train_indices = np.random.choice(len(train_images), train_num, replace=False)\n",
    "val_indices = np.random.choice(len(train_images), val_num, replace=False)\n",
    "\n",
    "def process_images(indices, image_count):\n",
    "    processed_images = []\n",
    "    for index in indices[:image_count]:\n",
    "        # padding\n",
    "        padded_image = np.pad(train_images[index], pad_width=2, mode='constant', constant_values=0)\n",
    "        # resize\n",
    "        # resized_image = resize_image(padded_image, (64, 64))\n",
    "        processed_images.append(padded_image)\n",
    "    return processed_images\n",
    "\n",
    "processed_train_images = process_images(train_indices, train_num)\n",
    "processed_val_images = process_images(val_indices, val_num)\n",
    "\n",
    "# first train image\n",
    "plt.imshow(processed_train_images[0], cmap='gray')\n",
    "plt.title('Processed Training Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# first val image\n",
    "plt.imshow(processed_val_images[0], cmap='gray')\n",
    "plt.title('Processed Validation Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:55.742168Z",
     "iopub.status.busy": "2024-09-04T22:03:55.741854Z",
     "iopub.status.idle": "2024-09-04T22:03:59.610281Z",
     "shell.execute_reply": "2024-09-04T22:03:59.609761Z",
     "shell.execute_reply.started": "2024-09-04T22:03:55.742149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Npat = 100 # nb of patterns on the SLM\n",
    "Nslm = 1024  # nb of SLM pixels\n",
    "Nfluo = 1024   # nb of beads\n",
    "Ncam = 1024  # nb of camera pixels\n",
    "\n",
    "magnitude = np.random.randn(Nfluo, Nslm)\n",
    "\n",
    "T1 = magnitude + 1j * magnitude\n",
    "T1_t = to_complex_tensor(T1)\n",
    "\n",
    "processed_train_images_flat = [img.reshape(-1) for img in processed_train_images] \n",
    "processed_val_images_flat = [img.reshape(-1) for img in processed_val_images]\n",
    "\n",
    "SLM_input_train = np.stack(processed_train_images_flat, axis=1) \n",
    "SLM_input_val = np.stack(processed_val_images_flat, axis=1)\n",
    "\n",
    "# to torch tensor\n",
    "SLM_input_train_pha = torch.from_numpy(SLM_input_train).float().to(device)\n",
    "SLM_input_train_t = phase_to_complex(SLM_input_train_pha)\n",
    "\n",
    "SLM_input_val_pha = torch.from_numpy(SLM_input_val).float().to(device)\n",
    "SLM_input_val_t = phase_to_complex(SLM_input_val_pha)\n",
    "\n",
    "field1_train_t = complex_matmul(T1_t, SLM_input_train_t)\n",
    "int1_train_t = torch.sum(torch.abs(field1_train_t)**2,-1)\n",
    "\n",
    "field1_val_t = complex_matmul(T1_t, SLM_input_val_t)\n",
    "int1_val_t = torch.sum(torch.abs(field1_val_t)**2,-1)\n",
    "# int1_val_t = int1_val_t.T\n",
    "# CAM_output_t = T2_t@int1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:59.611512Z",
     "iopub.status.busy": "2024-09-04T22:03:59.611199Z",
     "iopub.status.idle": "2024-09-04T22:03:59.639175Z",
     "shell.execute_reply": "2024-09-04T22:03:59.638744Z",
     "shell.execute_reply.started": "2024-09-04T22:03:59.611497Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(T1_t.cpu(), 'MNIST/train/T1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:03:59.640063Z",
     "iopub.status.busy": "2024-09-04T22:03:59.639761Z",
     "iopub.status.idle": "2024-09-04T22:03:59.657150Z",
     "shell.execute_reply": "2024-09-04T22:03:59.656740Z",
     "shell.execute_reply.started": "2024-09-04T22:03:59.640045Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# SLM_input_train was saved in NumPy \n",
    "np.save('MNIST/train/train_image.npy', SLM_input_train)\n",
    "\n",
    "# int1_train_t is a PyTorch tensor, so load to cpu\n",
    "torch.save(int1_train_t.cpu(), 'MNIST/train/train_speckle.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:00.055368Z",
     "iopub.status.busy": "2024-09-04T22:04:00.055003Z",
     "iopub.status.idle": "2024-09-04T22:04:00.073095Z",
     "shell.execute_reply": "2024-09-04T22:04:00.072677Z",
     "shell.execute_reply.started": "2024-09-04T22:04:00.055350Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('MNIST/val/val_image.npy', SLM_input_val)\n",
    "\n",
    "torch.save(int1_val_t.cpu(), 'MNIST/val/val_speckle.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:02.102448Z",
     "iopub.status.busy": "2024-09-04T22:04:02.102132Z",
     "iopub.status.idle": "2024-09-04T22:04:02.147815Z",
     "shell.execute_reply": "2024-09-04T22:04:02.147364Z",
     "shell.execute_reply.started": "2024-09-04T22:04:02.102430Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "array = np.load('MNIST/train/train_image.npy')\n",
    "\n",
    "print(int1_train_t)\n",
    "max_element = torch.max(int1_train_t)\n",
    "min_element = torch.min(int1_train_t)\n",
    "\n",
    "print(\"maximum:\", max_element.item())\n",
    "print(\"minimum:\", min_element.item())\n",
    "print(\"Shape of the array:\", array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:03.788795Z",
     "iopub.status.busy": "2024-09-04T22:04:03.788490Z",
     "iopub.status.idle": "2024-09-04T22:04:03.946546Z",
     "shell.execute_reply": "2024-09-04T22:04:03.946097Z",
     "shell.execute_reply.started": "2024-09-04T22:04:03.788777Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_row = int1_train_t.T.cpu()[0, :]  # data saved in row\n",
    "matrix_128x128 = first_row.reshape((32, 32))\n",
    "\n",
    "plt.imshow(matrix_128x128, cmap='viridis', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.title('Visualization of 128x128 Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:07.557040Z",
     "iopub.status.busy": "2024-09-04T22:04:07.556735Z",
     "iopub.status.idle": "2024-09-04T22:04:07.726035Z",
     "shell.execute_reply": "2024-09-04T22:04:07.725512Z",
     "shell.execute_reply.started": "2024-09-04T22:04:07.557022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = 'data/train/input'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# to .tif\n",
    "for idx, img in enumerate(processed_train_images):\n",
    "    # normalization\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    image = Image.fromarray(img)\n",
    "    file_path = os.path.join(output_dir, f'train_image_{idx}.png')\n",
    "    image.save(file_path)\n",
    "\n",
    "print(f\"Saved {len(processed_train_images)} images to {output_dir}\")\n",
    "\n",
    "file_name = 'train_image_0.png'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray')  # image saved in grayscale\n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:10.120993Z",
     "iopub.status.busy": "2024-09-04T22:04:10.120461Z",
     "iopub.status.idle": "2024-09-04T22:04:10.200642Z",
     "shell.execute_reply": "2024-09-04T22:04:10.200140Z",
     "shell.execute_reply.started": "2024-09-04T22:04:10.120975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = 'data/val/input'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, img in enumerate(processed_val_images):\n",
    "    img = (img - img.min()) / (img.max() - img.min())  \n",
    "    img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    image = Image.fromarray(img)\n",
    "    file_path = os.path.join(output_dir, f'val_image_{idx}.png')\n",
    "    image.save(file_path)\n",
    "\n",
    "print(f\"Saved {len(processed_val_images)} images to {output_dir}\")\n",
    "\n",
    "file_name = 'val_image_0.png'  \n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray')  # image saved in grayscale\n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:15.454979Z",
     "iopub.status.busy": "2024-09-04T22:04:15.454672Z",
     "iopub.status.idle": "2024-09-04T22:04:15.685767Z",
     "shell.execute_reply": "2024-09-04T22:04:15.685299Z",
     "shell.execute_reply.started": "2024-09-04T22:04:15.454960Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "speckle_data = torch.load('MNIST/train/train_speckle.npy')\n",
    "\n",
    "output_dir = 'data/train/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(speckle_data.shape[1]): \n",
    "    # save as n*n image\n",
    "    image = speckle_data[:, i].reshape(32, 32).numpy() \n",
    "    \n",
    "    # normalization\n",
    "    image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    # scaling\n",
    "    image_scaled = (image_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    output_filename = os.path.join(output_dir, f'train_speckle_{i}.png')\n",
    "    plt.imsave(output_filename, image_scaled, cmap='gray')\n",
    "\n",
    "file_name = 'train_speckle_0.png'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray', interpolation='none')\n",
    "plt.colorbar() \n",
    "plt.title('Visualization of 128x128 Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-27T18:52:54.414901Z",
     "iopub.status.busy": "2024-08-27T18:52:54.414565Z",
     "iopub.status.idle": "2024-08-27T18:52:54.445898Z",
     "shell.execute_reply": "2024-08-27T18:52:54.445339Z",
     "shell.execute_reply.started": "2024-08-27T18:52:54.414881Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_element = torch.max(image)\n",
    "min_element = torch.min(image)\n",
    "\n",
    "print(\"maximum:\", max_element.item())\n",
    "print(\"minimum:\", min_element.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:04:19.161108Z",
     "iopub.status.busy": "2024-09-04T22:04:19.160806Z",
     "iopub.status.idle": "2024-09-04T22:04:19.327224Z",
     "shell.execute_reply": "2024-09-04T22:04:19.326745Z",
     "shell.execute_reply.started": "2024-09-04T22:04:19.161090Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "speckle_data = torch.load('MNIST/val/val_speckle.npy')\n",
    "\n",
    "output_dir = 'data/val/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(speckle_data.shape[1]): \n",
    "    # save as n*n image\n",
    "    image = speckle_data[:, i].reshape(32, 32).numpy()  \n",
    "    \n",
    "    image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    image_scaled = (image_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    output_filename = os.path.join(output_dir, f'val_speckle_{i}.png')\n",
    "    plt.imsave(output_filename, image_scaled, cmap='gray')\n",
    "\n",
    "file_name = 'val_speckle_0.png' \n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray', interpolation='none')\n",
    "plt.colorbar() \n",
    "plt.title('Visualization of 128x128 Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-30T16:46:52.586260Z",
     "iopub.status.busy": "2024-08-30T16:46:52.585922Z",
     "iopub.status.idle": "2024-08-30T16:46:52.650669Z",
     "shell.execute_reply": "2024-08-30T16:46:52.650156Z",
     "shell.execute_reply.started": "2024-08-30T16:46:52.586237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dir_path = 'data/train/noised_gaussian_150_L'\n",
    "\n",
    "if os.path.exists(dir_path):\n",
    "    # Remove all files and folders in the directory\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove file or link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # Remove directory\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "else:\n",
    "    print(f'The directory {dir_path} does not exist.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-01T01:39:45.138478Z",
     "iopub.status.busy": "2024-08-01T01:39:45.137831Z",
     "iopub.status.idle": "2024-08-01T01:39:47.767691Z",
     "shell.execute_reply": "2024-08-01T01:39:47.767054Z",
     "shell.execute_reply.started": "2024-08-01T01:39:45.138456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Noise probability\n",
    "p = 0.3\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = 'data/train/output'\n",
    "output_dir = 'data/train/noised_03'\n",
    "mask_dir = os.path.join(output_dir, 'masks')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Get all image files\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "\n",
    "# Process each image\n",
    "for filename in image_files:\n",
    "    # Read the image and convert to grayscale\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = np.array(Image.open(img_path).convert('L'))  # 'L' mode for grayscale\n",
    "    \n",
    "    # Create a random matrix and apply noise\n",
    "    mask = np.random.rand(*image.shape) < p\n",
    "    noised_image = image.copy()\n",
    "    noised_image[mask] = 0\n",
    "    \n",
    "    # Save the processed image\n",
    "    output_img_path = os.path.join(output_dir, f'train_noise_{filename.split(\"_\")[-1]}')\n",
    "    Image.fromarray(noised_image).save(output_img_path)\n",
    "    \n",
    "    # Save the mask (0 where noise was added, 1 elsewhere)\n",
    "    mask_save = np.where(mask, 0, 1)\n",
    "    mask_filename = f'mask_{filename.split(\"_\")[-1]}'\n",
    "    mask_filename = mask_filename.replace('.png', '.npy')\n",
    "    mask_img_path = os.path.join(mask_dir, mask_filename)\n",
    "    np.save(mask_img_path, mask_save)  # Save the mask as a numpy array\n",
    "\n",
    "# Assuming we want to display the first image and its mask\n",
    "file_name = 'train_noise_0.png'\n",
    "mask_name = 'mask_0.npy'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "# Load the image and the mask\n",
    "image = Image.open(file_path)\n",
    "mask = np.load(mask_path)  # Load the mask numpy array\n",
    "\n",
    "# Display the image and the mask\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask, cmap='gray')  # Mask might look fully white if most values are 1\n",
    "plt.title(f'Displaying {mask_name}')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:05:58.681833Z",
     "iopub.status.busy": "2024-09-04T22:05:58.681267Z",
     "iopub.status.idle": "2024-09-04T22:05:59.245015Z",
     "shell.execute_reply": "2024-09-04T22:05:59.244538Z",
     "shell.execute_reply.started": "2024-09-04T22:05:58.681815Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "\n",
    "def add_gaussian_noise_and_complex_adjust(img, field_column, model_path, sigma):\n",
    "    img = torch.from_numpy(img).to(torch.float32).to('cuda')  # Convert image to PyTorch tensor and move to GPU\n",
    "    print(img[1, :])\n",
    "    index = model_path.rfind(\"/\")\n",
    "    if sigma > 0:\n",
    "        noise = torch.normal(mean=0.0, std=sigma / 255., size=img.size(), device='cuda')\n",
    "        # noise = torch.clamp(noise, -0.5, 0.5)\n",
    "        print(noise[1, :])\n",
    "        sio.savemat(model_path[:index] + '/noise.mat', {'noise': noise.cpu().numpy()})  # Save noise to file\n",
    "        noisy_img = img + noise\n",
    "    else:\n",
    "        noisy_img = img\n",
    "\n",
    "    max_value = torch.max(img)  \n",
    "    noisy_img_clipped = torch.clamp(noisy_img, 0, max_value).to(torch.uint8)\n",
    "    print(noisy_img_clipped[1, :])\n",
    "    cv2.imwrite(model_path[:index] + '/noisy.png', noisy_img_clipped.cpu().numpy().squeeze()) \n",
    "    \n",
    "    # Adjust the complex field based on noise\n",
    "    field = field_column.reshape(32, 32, 2).to(torch.float32).to('cuda')\n",
    "    intensity = torch.sum(torch.abs(field) ** 2, dim=-1)\n",
    "    new_intensity = intensity + noise.reshape(32, 32)\n",
    "    \n",
    "    magnitude = torch.sqrt(new_intensity)\n",
    "    phase = torch.atan2(field[:,:,1], field[:,:,0])\n",
    "    new_field = torch.stack((magnitude * torch.cos(phase), magnitude * torch.sin(phase)), dim=-1)\n",
    "    \n",
    "    return noisy_img.cpu().numpy(), new_field.cpu().numpy()  # Move results to CPU and convert to NumPy\n",
    "\n",
    "# Configure the noise parameters and directories\n",
    "sigma = 150  # Standard deviation for Gaussian noise\n",
    "input_dir = 'data/train/output'\n",
    "output_dir = 'data/train/noised_gaussian_150_L'\n",
    "output_complex_dir = 'data/train/output_complex'\n",
    "noise_dir = os.path.join(output_dir, 'noise_matrices')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(noise_dir, exist_ok=True)\n",
    "os.makedirs(output_complex_dir, exist_ok=True)\n",
    "\n",
    "# Process each image file\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "for idx, filename in enumerate(image_files):\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = np.array(Image.open(img_path).convert('L'), dtype=np.float32)  # Load image and convert to grayscale\n",
    "    model_path = os.path.join(noise_dir, filename)\n",
    "    noised_image, new_field = add_gaussian_noise_and_complex_adjust(image, field1_train_t[:, idx, :], model_path, sigma)\n",
    "    \n",
    "    # Save the processed image and complex field\n",
    "    output_img_path = os.path.join(output_dir, f'gaussian_noised_{filename}')\n",
    "    Image.fromarray(noised_image.astype(np.uint8)).save(output_img_path)\n",
    "    np.save(os.path.join(output_complex_dir, f'complex_{filename[:-4]}.npy'), new_field)\n",
    "\n",
    "print(\"Gaussian noise added to all images and new complex matrices saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:15:25.652733Z",
     "iopub.status.busy": "2024-09-04T22:15:25.652405Z",
     "iopub.status.idle": "2024-09-04T22:15:26.260829Z",
     "shell.execute_reply": "2024-09-04T22:15:26.260269Z",
     "shell.execute_reply.started": "2024-09-04T22:15:25.652711Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "\n",
    "def add_gaussian_noise_and_complex_adjust(img, field_column, base_path, sigma, noise_count=10):\n",
    "    img_tensor = torch.from_numpy(img).to(torch.float32).to('cuda')\n",
    "    noise_list = []\n",
    "    \n",
    "    # Create directories for each noise mask\n",
    "    noise_dirs = [os.path.join(base_path, str(i + 1)) for i in range(noise_count)]\n",
    "    for dir_path in noise_dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    for i, noise_dir in enumerate(noise_dirs):\n",
    "        noise = torch.normal(mean=0.0, std=sigma / 255., size=img_tensor.size(), device='cuda')\n",
    "        noise_list.append(noise)\n",
    "        noisy_img = img_tensor + noise\n",
    "        noisy_img_clipped = torch.clamp(noisy_img, 0, 255).to(torch.uint8)\n",
    "        cv2.imwrite(os.path.join(noise_dir, 'noisy.png'), noisy_img_clipped.cpu().numpy().squeeze())\n",
    "\n",
    "    # Compute and handle the average noise\n",
    "    average_noise = torch.mean(torch.stack(noise_list), dim=0)\n",
    "    sio.savemat(base_path + '/average_noise.mat', {'noise': average_noise.cpu().numpy()})\n",
    "    noisy_img_avg = img_tensor + average_noise\n",
    "    noisy_img_avg_clipped = torch.clamp(noisy_img_avg, 0, 255).to(torch.uint8)\n",
    "    cv2.imwrite(os.path.join(base_path, 'average_noised.png'), noisy_img_avg_clipped.cpu().numpy().squeeze())\n",
    "\n",
    "    # Adjust the complex field based on average noise\n",
    "    field = field_column.reshape(32, 32, 2).to(torch.float32).to('cuda')\n",
    "    intensity = torch.sum(torch.abs(field) ** 2, dim=-1)\n",
    "    new_intensity = intensity + average_noise.reshape(32, 32)\n",
    "    magnitude = torch.sqrt(new_intensity)\n",
    "    phase = torch.atan2(field[:, :, 1], field[:, :, 0])\n",
    "    new_field = torch.stack((magnitude * torch.cos(phase), magnitude * torch.sin(phase)), dim=-1)\n",
    "\n",
    "    return noisy_img_avg_clipped.cpu().numpy(), new_field.cpu().numpy()\n",
    "\n",
    "sigma = 150  # Standard deviation for Gaussian noise\n",
    "input_dir = 'data/train/output'\n",
    "output_dir = 'data/train/noised_gaussian_150_L'\n",
    "output_complex_dir = 'data/train/output_complex'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(output_complex_dir, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "for idx, filename in enumerate(image_files):\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = np.array(Image.open(img_path).convert('L'), dtype=np.float32)\n",
    "    base_path = os.path.join(output_dir, filename[:-4])  # Remove '.png' and use as base path\n",
    "    noised_image_avg, new_field = add_gaussian_noise_and_complex_adjust(image, field1_train_t[:, idx, :], base_path, sigma)\n",
    "    \n",
    "    # Save the average noised image and complex field\n",
    "    output_img_path = os.path.join(output_dir, f'average_gaussian_noised_{filename}')\n",
    "    Image.fromarray(noised_image_avg.astype(np.uint8)).save(output_img_path)\n",
    "    np.save(os.path.join(output_complex_dir, f'complex_avg_{filename[:-4]}.npy'), new_field)\n",
    "\n",
    "print(\"Gaussian noise added to all images with multiple masks and averaged. New complex matrices saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-27T23:46:59.620415Z",
     "iopub.status.busy": "2024-08-27T23:46:59.620091Z",
     "iopub.status.idle": "2024-08-27T23:46:59.676406Z",
     "shell.execute_reply": "2024-08-27T23:46:59.675951Z",
     "shell.execute_reply.started": "2024-08-27T23:46:59.620397Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "\n",
    "def add_poisson_noise(img, model_path, lam):\n",
    "    index = model_path.rfind(\"/\")\n",
    "    lam = lam / 255\n",
    "    if lam > 0:\n",
    "        noise = np.random.poisson(lam, size=img.shape).astype(np.float32) - lam\n",
    "        print(noise[1, :])\n",
    "        sio.savemat(model_path[0:index] + '/noise.mat', {'noise': noise})\n",
    "        noisy_img = img + noise\n",
    "    else:\n",
    "        noisy_img = img.astype(np.float32)\n",
    "    cv2.imwrite(model_path[0:index] + '/noisy.png', np.squeeze(np.int32(np.clip(noisy_img, 0, 255))))\n",
    "    return noisy_img\n",
    "\n",
    "# Noise parameters\n",
    "lam = 240  # Lambda parameter for the Poisson distribution\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = 'data/val/output'\n",
    "output_dir = 'data/train/noised_poisson_240'\n",
    "noise_dir = os.path.join(output_dir, 'noise_matrices')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(noise_dir, exist_ok=True)\n",
    "\n",
    "# Get all image files\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "\n",
    "# Process each image\n",
    "for filename in image_files:\n",
    "    # Read the image\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = np.array(Image.open(img_path).convert('L'), dtype=np.float32)  # Convert image to grayscale for simplicity\n",
    "    \n",
    "    # Model path for saving noise matrix and noisy image\n",
    "    model_path = os.path.join(noise_dir, filename)\n",
    "    \n",
    "    # Add Poisson noise to the image\n",
    "    noised_image = add_poisson_noise(image, model_path, lam)\n",
    "    \n",
    "    # Save the processed image\n",
    "    output_img_path = os.path.join(output_dir, f'poisson_noised_{filename}')\n",
    "    Image.fromarray(np.uint8(noised_image)).save(output_img_path)\n",
    "\n",
    "print(\"Poisson noise added to all images and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-27T08:35:31.669806Z",
     "iopub.status.busy": "2024-08-27T08:35:31.669470Z",
     "iopub.status.idle": "2024-08-27T08:35:31.709740Z",
     "shell.execute_reply": "2024-08-27T08:35:31.709087Z",
     "shell.execute_reply.started": "2024-08-27T08:35:31.669788Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#file_path = 'data/train/output_corr_0/complex_train_speckle_0.npy'\n",
    "file_path = 'MNIST/train/train_speckle.npy'\n",
    "\n",
    "data = torch.load(file_path)\n",
    "\n",
    "print(\"Shape of the data:\", data.shape)\n",
    "print(\"Data type of the data:\", data.dtype)\n",
    "\n",
    "print(\"First few elements of the data:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-22T08:32:48.923033Z",
     "iopub.status.busy": "2024-08-22T08:32:48.922656Z",
     "iopub.status.idle": "2024-08-22T08:35:44.896766Z",
     "shell.execute_reply": "2024-08-22T08:35:44.896177Z",
     "shell.execute_reply.started": "2024-08-22T08:32:48.923011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def compute_mu(field):\n",
    "    N = field.shape[0]\n",
    "    M = field.shape[1]\n",
    "    mu = np.zeros((N, M), dtype=np.float32)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            if i == j:\n",
    "                continue  # diag\n",
    "            R_i, L_i = field[i, j, 0], field[i, j, 1]\n",
    "            R_j, L_j = field[j, i, 0], field[j, i, 1]\n",
    "            \n",
    "            numerator = R_i * R_j + L_i * L_j\n",
    "            denominator = np.sqrt((R_i**2 + L_i**2) * (R_j**2 + L_j**2))\n",
    "            if denominator != 0:\n",
    "                mu[i, j] = numerator / denominator\n",
    "    \n",
    "    return mu\n",
    "\n",
    "input_dir = 'data/train/output_complex'\n",
    "output_dir = 'data/train/output_corr_0'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        complex_field = np.load(file_path)\n",
    "        \n",
    "        mu_matrix = compute_mu(complex_field.reshape(128, 128, 2)) \n",
    "        \n",
    "        output_file_path = os.path.join(output_dir, filename)\n",
    "        np.save(output_file_path, mu_matrix)\n",
    "\n",
    "print(\"Correlation matrices (mu values) have been calculated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-26T21:04:11.197320Z",
     "iopub.status.busy": "2024-08-26T21:04:11.196924Z",
     "iopub.status.idle": "2024-08-26T21:07:16.692362Z",
     "shell.execute_reply": "2024-08-26T21:07:16.691815Z",
     "shell.execute_reply.started": "2024-08-26T21:04:11.197299Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cupy as cp\n",
    "\n",
    "def compute_mu(field):\n",
    "    N = field.shape[0] * field.shape[1] \n",
    "    flat_field = field.reshape(N, 2)  # flatten matrix\n",
    "    flat_field = cp.asarray(flat_field)\n",
    "    \n",
    "    mu = cp.zeros((N, N - 1), dtype=cp.float32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        print(i)\n",
    "        k = 0  \n",
    "        for j in range(N):\n",
    "            if i != j:\n",
    "                R_i, L_i = flat_field[i]\n",
    "                R_j, L_j = flat_field[j]\n",
    "                \n",
    "                numerator = R_i * R_j + L_i * L_j\n",
    "                denominator = cp.sqrt((R_i**2 + L_i**2) * (R_j**2 + L_j**2))\n",
    "                if denominator != 0:\n",
    "                    mu[i, k] = numerator / denominator\n",
    "                k += 1\n",
    "    \n",
    "    mu = cp.asnumpy(mu)\n",
    "    return mu\n",
    "\n",
    "input_dir = 'data/train/output_complex_0'\n",
    "output_dir = 'data/train/output_corr_0'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        complex_field = np.load(file_path)\n",
    "        complex_field = cp.asarray(complex_field)\n",
    "        \n",
    "        mu_matrix = compute_mu(complex_field.reshape(32, 32, 2))  \n",
    "        \n",
    "        output_file_path = os.path.join(output_dir, filename)\n",
    "        np.save(output_file_path, mu_matrix)\n",
    "\n",
    "print(\"Correlation matrices (mu values) have been calculated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T22:56:51.204839Z",
     "iopub.status.busy": "2024-09-04T22:56:51.204508Z",
     "iopub.status.idle": "2024-09-04T23:00:36.708785Z",
     "shell.execute_reply": "2024-09-04T23:00:36.708289Z",
     "shell.execute_reply.started": "2024-09-04T22:56:51.204819Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "def compute_mu(field):\n",
    "    N = field.shape[0] * field.shape[1] \n",
    "    flat_field = field.reshape(N, 2)\n",
    "\n",
    "    flat_field = cp.asarray(flat_field)\n",
    "    mu = cp.zeros((N, N), dtype=cp.float32)\n",
    "    \n",
    "    cp.fill_diagonal(mu, 1)\n",
    "\n",
    "    for i in range(N):\n",
    "        print(i)\n",
    "        for j in range(i+1, N):  # calculate only upper triangular matrix\n",
    "            R_i, L_i = flat_field[i]\n",
    "            R_j, L_j = flat_field[j]\n",
    "            \n",
    "            numerator = R_i * R_j + L_i * L_j\n",
    "            denominator = cp.sqrt((R_i**2 + L_i**2) * (R_j**2 + L_j**2))\n",
    "            if denominator != 0:\n",
    "                mu[i, j] = mu[j, i] = numerator / denominator\n",
    "    \n",
    "    mu = cp.asnumpy(mu)\n",
    "    return mu\n",
    "\n",
    "input_dir = 'data/train/output_complex_0'\n",
    "output_dir = 'data/train/output_corr_00'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        complex_field = np.load(file_path)\n",
    "        complex_field = cp.asarray(complex_field)\n",
    "        \n",
    "        mu_matrix = compute_mu(complex_field.reshape(32, 32, 2)) \n",
    "        \n",
    "        output_file_path = os.path.join(output_dir, filename)\n",
    "        np.save(output_file_path, mu_matrix)\n",
    "\n",
    "print(\"Correlation matrices (mu values) have been calculated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-28T00:37:18.636801Z",
     "iopub.status.busy": "2024-08-28T00:37:18.636191Z",
     "iopub.status.idle": "2024-08-28T00:37:20.305170Z",
     "shell.execute_reply": "2024-08-28T00:37:20.304707Z",
     "shell.execute_reply.started": "2024-08-28T00:37:18.636783Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.special import i0  # Bessel function of the first kind\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = Image.open(file_path).convert('L')  # grayscale\n",
    "    img_array = np.array(img)\n",
    "    print(\"Original image shape:\", img_array.shape)  # size of the original images\n",
    "    \n",
    "    # remove even columns\n",
    "    processed_array = img_array[:, 1::2]\n",
    "    processed_array = np.clip(processed_array, 1e-8, 255)\n",
    "    # flatten\n",
    "    flattened_vector = processed_array.flatten()\n",
    "    print(\"Processed image shape:\", processed_array.shape)  # image size after processing\n",
    "    print(\"Flattened vector length:\", len(flattened_vector))  # length of flattened vector\n",
    "    return flattened_vector\n",
    "\n",
    "def process_matrix(file_path):\n",
    "    matrix = np.load(file_path)\n",
    "    \n",
    "    # remove odd columns and even rows\n",
    "    processed_matrix = matrix[1::2, ::2]\n",
    "    print(\"Processed matrix shape:\", processed_matrix.shape)\n",
    "\n",
    "    lower_bound = -1 + 1e-8\n",
    "    upper_bound = 1 - 1e-8\n",
    "    processed_matrix = np.clip(processed_matrix, lower_bound, upper_bound)\n",
    "    \n",
    "    return processed_matrix\n",
    "\n",
    "def calculate_probability(vector, matrix):\n",
    "    probabilities = []\n",
    "    \n",
    "    for i, I_i in enumerate(vector):\n",
    "        M_i = matrix[i, :]  # ith row\n",
    "        numerator = np.exp(- (1 + M_i**2) / (1 - M_i**2)) * i0(2 * M_i / (1 - M_i**2))\n",
    "        denominator = I_i * (1 - M_i**2)\n",
    "            \n",
    "        p_i_M = numerator / denominator\n",
    "        p_i_M[np.isinf(p_i_M)] = np.nan\n",
    "        p_i_M = np.clip(p_i_M, -1, 1)\n",
    "        p_i_M_avg = np.nanmean(p_i_M)  # avg prob of the ith row, ignore NaN and inf\n",
    "        probabilities.append(p_i_M_avg)\n",
    "    \n",
    "    return np.nanmean(probabilities)  # avg prob, ignore NaN and inf\n",
    "\n",
    "\n",
    "image_dir = 'data/train/output_temp'\n",
    "matrix_dir = 'data/train/output_corr'\n",
    "results = []\n",
    "   \n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "matrix_files = sorted([f for f in os.listdir(matrix_dir) if f.endswith('.npy')])\n",
    "    \n",
    "matches = {img_file: None for img_file in image_files}\n",
    "for img_file in image_files:\n",
    "    index = img_file.split('_')[-1].split('.')[0]  # indices after _k\n",
    "    for mat_file in matrix_files:\n",
    "        if mat_file.endswith(f'_{index}.npy'):\n",
    "            matches[img_file] = mat_file\n",
    "            break\n",
    "    \n",
    "for img_file, mat_file in matches.items():\n",
    "    vector = process_image(os.path.join(image_dir, img_file))\n",
    "    matrix = process_matrix(os.path.join(matrix_dir, mat_file))\n",
    "        \n",
    "    probability = calculate_probability(vector, matrix)\n",
    "    results.append(probability)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T23:23:23.227257Z",
     "iopub.status.busy": "2024-09-04T23:23:23.226804Z",
     "iopub.status.idle": "2024-09-04T23:23:24.469960Z",
     "shell.execute_reply": "2024-09-04T23:23:24.469301Z",
     "shell.execute_reply.started": "2024-09-04T23:23:23.227235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.special import i0  # Bessel function of the first kind\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = Image.open(file_path).convert('L')  # grayscale\n",
    "    img_array = np.array(img)\n",
    "    print(\"Original image shape:\", img_array.shape) \n",
    "    \n",
    "    # remove even columns\n",
    "    processed_array = img_array[:, 1::2]\n",
    "    processed_array = np.clip(processed_array, 1e-8, 255)\n",
    "    flattened_vector = processed_array.flatten()\n",
    "    print(\"Processed image shape:\", processed_array.shape) \n",
    "    print(\"Flattened vector length:\", len(flattened_vector)) \n",
    "    return flattened_vector\n",
    "\n",
    "def process_matrix(file_path):\n",
    "    matrix = np.load(file_path)\n",
    "    \n",
    "    # remove odd columns and even rows\n",
    "    processed_matrix = matrix[1::2, ::2]\n",
    "    print(\"Processed matrix shape:\", processed_matrix.shape)\n",
    "\n",
    "    lower_bound = -1 + 1e-8\n",
    "    upper_bound = 1 - 1e-8\n",
    "    processed_matrix = np.clip(processed_matrix, lower_bound, upper_bound)\n",
    "    \n",
    "    return processed_matrix\n",
    "\n",
    "def calculate_probability(vector, matrix):\n",
    "    probabilities = []\n",
    "    \n",
    "    for i, I_i in enumerate(vector):\n",
    "        M_i = matrix[i, :]  # ith row\n",
    "        numerator = np.exp(- (1 + M_i**2) / (1 - M_i**2)) * i0(2 * M_i / (1 - M_i**2))\n",
    "        denominator = I_i * (1 - M_i**2)\n",
    "            \n",
    "        p_i_M = numerator / denominator\n",
    "        p_i_M[np.isinf(p_i_M)] = np.nan\n",
    "        p_i_M = np.clip(p_i_M, -1, 1)\n",
    "        p_i_M_avg = np.nanmean(p_i_M)  # avg prob of the ith row, ignore NaN and inf\n",
    "        probabilities.append(p_i_M_avg)\n",
    "    \n",
    "    return np.nanmean(probabilities)  # avg prob\n",
    "\n",
    "\n",
    "image_dir = 'data/train/avgnoise_pred'\n",
    "matrix_dir = 'data/train/output_corr_0'\n",
    "results = []\n",
    "   \n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "matrix_files = sorted([f for f in os.listdir(matrix_dir) if f.endswith('.npy')])\n",
    "    \n",
    "matches = {img_file: None for img_file in image_files}\n",
    "for img_file in image_files:\n",
    "    index = img_file.split('_')[-1].split('.')[0]  \n",
    "    for mat_file in matrix_files:\n",
    "        if mat_file.endswith(f'_{index}.npy'):\n",
    "            matches[img_file] = mat_file\n",
    "            break\n",
    "    \n",
    "for img_file, mat_file in matches.items():\n",
    "    vector = process_image(os.path.join(image_dir, img_file))\n",
    "    matrix = process_matrix(os.path.join(matrix_dir, mat_file))\n",
    "        \n",
    "    probability = calculate_probability(vector, matrix)\n",
    "    results.append((img_file, probability)) \n",
    "\n",
    "for img_name, result in results:\n",
    "    print(f\"{img_name}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-04T23:37:37.556983Z",
     "iopub.status.busy": "2024-09-04T23:37:37.556694Z",
     "iopub.status.idle": "2024-09-04T23:37:37.701074Z",
     "shell.execute_reply": "2024-09-04T23:37:37.700595Z",
     "shell.execute_reply.started": "2024-09-04T23:37:37.556964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data1 = np.loadtxt('prob.txt', delimiter=',')\n",
    "data2 = np.loadtxt('prob_2.txt', delimiter=',')\n",
    "\n",
    "x = data1[:, 0] \n",
    "y1 = data1[:, 1]  \n",
    "y2 = data2[:, 1]  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y1, marker='o', linestyle='-', color='blue', label='prob.txt')\n",
    "plt.plot(x, y2, marker='x', linestyle='--', color='red', label='prob_2.txt')\n",
    "plt.title('Probability Comparisons')\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-30T00:17:19.389172Z",
     "iopub.status.busy": "2024-08-30T00:17:19.388829Z",
     "iopub.status.idle": "2024-08-30T00:17:19.472807Z",
     "shell.execute_reply": "2024-08-30T00:17:19.472224Z",
     "shell.execute_reply.started": "2024-08-30T00:17:19.389152Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.special import i0  # Bessel function of the first kind\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = Image.open(file_path).convert('L')  # grayscale\n",
    "    img_array = np.array(img)\n",
    "    print(\"Original image shape:\", img_array.shape)  \n",
    "    \n",
    "    # remove even columns\n",
    "    processed_array = img_array[:, 1::2]\n",
    "    processed_array = np.clip(processed_array, 1e-8, 255)\n",
    "\n",
    "    flattened_vector = processed_array.flatten()\n",
    "    print(\"Processed image shape:\", processed_array.shape)  \n",
    "    print(\"Flattened vector length:\", len(flattened_vector)) \n",
    "    return flattened_vector\n",
    "\n",
    "def process_matrix(file_path):\n",
    "    matrix = np.load(file_path)\n",
    "    \n",
    "    # remove odd columns and even rows\n",
    "    processed_matrix = matrix[1::2, ::2]\n",
    "    print(\"Processed matrix shape:\", processed_matrix.shape)\n",
    "\n",
    "    lower_bound = -1 + 1e-8\n",
    "    upper_bound = 1 - 1e-8\n",
    "    processed_matrix = np.clip(processed_matrix, lower_bound, upper_bound)\n",
    "    \n",
    "    return processed_matrix\n",
    "\n",
    "def calculate_probability(vector, matrix):\n",
    "    probabilities = []\n",
    "    \n",
    "    for i, I_i in enumerate(vector):\n",
    "        M_i = matrix[i, :]  \n",
    "        numerator = np.exp(- (1 + M_i**2) / (1 - M_i**2)) * i0(2 * M_i / (1 - M_i**2))\n",
    "        denominator = I_i * (1 - M_i**2)\n",
    "            \n",
    "        p_i_M = numerator / denominator\n",
    "        p_i_M[np.isinf(p_i_M)] = np.nan\n",
    "        p_i_M = np.clip(p_i_M, -1, 1)\n",
    "        p_i_M_avg = np.nanmean(p_i_M)  \n",
    "        probabilities.append(p_i_M_avg)\n",
    "    \n",
    "    return np.nanmean(probabilities)  \n",
    "\n",
    "\n",
    "image_dir = 'data/train/noise_pred'\n",
    "matrix_dir = 'data/train/output_corr_0'\n",
    "results = []\n",
    "   \n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "matrix_files = sorted([f for f in os.listdir(matrix_dir) if f.endswith('.npy')])\n",
    "    \n",
    "matches = {img_file: None for img_file in image_files}\n",
    "for img_file in image_files:\n",
    "    index = img_file.split('_')[-1].split('.')[0]\n",
    "    for mat_file in matrix_files:\n",
    "        if mat_file.endswith(f'_{index}.npy'):\n",
    "            matches[img_file] = mat_file\n",
    "            break\n",
    "    \n",
    "for img_file, mat_file in matches.items():\n",
    "    vector = process_image(os.path.join(image_dir, img_file))\n",
    "    matrix = process_matrix(os.path.join(matrix_dir, mat_file))\n",
    "        \n",
    "    probability = calculate_probability(vector, matrix)\n",
    "    results.append(probability)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T22:25:26.204658Z",
     "iopub.status.busy": "2024-08-22T22:25:26.204287Z",
     "iopub.status.idle": "2024-08-22T22:31:47.282279Z",
     "shell.execute_reply": "2024-08-22T22:31:47.281730Z",
     "shell.execute_reply.started": "2024-08-22T22:25:26.204639Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def bessel_I0(z):\n",
    "    if z <= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(z) / np.sqrt(2 * np.pi * z)\n",
    "\n",
    "def compute_pdf(image_path, mu_matrix_path):\n",
    "    image = np.array(Image.open(image_path).convert('L'), dtype=np.float32)\n",
    "    mu_matrix = np.load(mu_matrix_path)\n",
    "    \n",
    "    N = 128  # calculate only first 128 pixels\n",
    "    probabilities = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        prob_sum = 0\n",
    "        count = 0\n",
    "        for j in range(16384): # 128*128 \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            I_i = image[i // image.shape[1], i % image.shape[1]]\n",
    "            I_j = image[j // image.shape[1], j % image.shape[1]]\n",
    "            mu_ij = mu_matrix[i // image.shape[1], j % image.shape[1]]\n",
    "            if mu_ij <= -1:\n",
    "                mu_ij = -0.99\n",
    "            if mu_ij >= 1:\n",
    "                mu_ij = 0.99\n",
    "            \n",
    "            if I_j == 0 or 1 - mu_ij**2 == 0:\n",
    "                continue  \n",
    "            \n",
    "            exp_argument = (1 + mu_ij**2) / (1 - mu_ij**2)\n",
    "            z = 2 * mu_ij / (1 - mu_ij**2)\n",
    "            I0_val = bessel_I0(z)\n",
    "            p = 1 / (I_j * (1 - mu_ij**2)) * np.exp(-exp_argument) * I0_val\n",
    "            \n",
    "            if np.isfinite(p): \n",
    "                prob_sum += p\n",
    "                count += 1\n",
    "\n",
    "        probabilities[i] = prob_sum / count if count > 0 else 0\n",
    "\n",
    "    # avg prob excluding NaN and inf\n",
    "    valid_probs = probabilities[np.isfinite(probabilities)]\n",
    "    valid_prob_avg = np.mean(valid_probs) if valid_probs.size > 0 else 0\n",
    "    return valid_prob_avg, probabilities\n",
    "\n",
    "input_dir = 'data/train/noised_gaussian_150_L'\n",
    "mu_dir = 'data/train/output_corr'\n",
    "output_probabilities = []\n",
    "probs = []\n",
    "\n",
    "# avg prob for each image\n",
    "for x in range(2,10):\n",
    "    image_path = os.path.join(input_dir, f'gaussian_noised_train_speckle_{x}.png')\n",
    "    mu_path = os.path.join(mu_dir, f'complex_train_speckle_{x}.npy')\n",
    "    avg_valid_prob, prob = compute_pdf(image_path, mu_path)\n",
    "    output_probabilities.append(avg_valid_prob)\n",
    "    probs.append(prob)\n",
    "\n",
    "for idx, avg_prob in enumerate(output_probabilities):\n",
    "    print(f\"Average valid conditional probabilities for image {idx}: {avg_prob}\")\n",
    "    \n",
    "for idx, pdf in enumerate(probs):\n",
    "    print(f\"Average conditional probabilities for image {idx}: {pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:05:08.205972Z",
     "iopub.status.busy": "2024-08-21T23:05:08.205628Z",
     "iopub.status.idle": "2024-08-21T23:05:08.219202Z",
     "shell.execute_reply": "2024-08-21T23:05:08.218617Z",
     "shell.execute_reply.started": "2024-08-21T23:05:08.205954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bessel_I0(x):\n",
    "    \"\"\"Approximate the modified Bessel function of the first kind, I_0(x), using a series expansion.\"\"\"\n",
    "    terms = 10  # Number of terms in the series expansion\n",
    "    result = tf.ones_like(x)\n",
    "    factorial = tf.ones_like(x)\n",
    "    x_squared = tf.square(x / 2)\n",
    "    power_of_x_squared = tf.ones_like(x)\n",
    "    \n",
    "    for k in range(1, terms + 1):\n",
    "        factorial *= k\n",
    "        power_of_x_squared *= x_squared\n",
    "        result += power_of_x_squared / (factorial * factorial)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_conditional_probabilities(response, mask_tensor, mu_matrix):\n",
    "    mu_matrix = tf.convert_to_tensor(mu_matrix, dtype=tf.float32)\n",
    "    \n",
    "    # Expand dimensions to match the response tensor shape for broadcasting\n",
    "    mu_matrix = tf.expand_dims(mu_matrix, 0)  # Assuming batch dimension is required\n",
    "    \n",
    "    # Get dimensions for indexing and calculations\n",
    "    N = tf.shape(response)[-1]  # assuming the last dimension is the spatial dimension\n",
    "\n",
    "    # Flatten response and mask to simplify the calculation\n",
    "    response_flat = tf.reshape(response, [-1, N])\n",
    "    mask_flat = tf.reshape(mask_tensor, [-1, N])\n",
    "\n",
    "    # Compute probabilities for all pairs (i, j)\n",
    "    # Prepare mu_ij values, ensuring they are within the limits [-1 + eps, 1 - eps] to avoid division by zero in 1 - mu_ij^2\n",
    "    eps = 1e-6\n",
    "    mu_ij = tf.clip_by_value(mu_matrix, -1 + eps, 1 - eps)\n",
    "\n",
    "    exp_argument = (1 + mu_ij**2) / (1 - mu_ij**2)\n",
    "    z = 2 * mu_ij / (1 - mu_ij**2)\n",
    "    I0_val = bessel_I0(z)\n",
    "\n",
    "    # Calculate the actual probabilities\n",
    "    I_j = tf.expand_dims(response_flat, 2)  # Create a new axis for j\n",
    "    probabilities = 1 / (I_j * (1 - mu_ij**2)) * tf.exp(-exp_argument) * I0_val\n",
    "    \n",
    "    # Use mask to adjust where j is not used in the computation\n",
    "    mask_j = tf.expand_dims(mask_flat, 2)  # Apply mask\n",
    "    probabilities *= mask_j  # Apply mask to exclude certain values\n",
    "\n",
    "    # Check for finite probabilities and ignore NaNs or Infs\n",
    "    probabilities = tf.where(tf.math.is_finite(probabilities), probabilities, tf.zeros_like(probabilities))\n",
    "\n",
    "    # Sum probabilities over all j for each i, ignoring i itself (diagonal elements)\n",
    "    diagonal_mask = 1 - tf.eye(N)  # Create a mask to zero-out diagonal (self-references)\n",
    "    diagonal_mask = tf.expand_dims(diagonal_mask, 0)  # Adjust shape for batch\n",
    "    probabilities *= diagonal_mask  # Apply diagonal mask\n",
    "\n",
    "    # Average the probabilities over all j for each i\n",
    "    probabilities_sum = tf.reduce_sum(probabilities, axis=2)\n",
    "    count_valid = tf.reduce_sum(diagonal_mask * mask_j, axis=2)  # count valid terms only\n",
    "    avg_probabilities = probabilities_sum / count_valid\n",
    "\n",
    "    # Finally, average over all i to get a single probability value per image in the batch\n",
    "    final_prob = tf.reduce_mean(avg_probabilities, axis=1)\n",
    "\n",
    "    return final_prob\n",
    "\n",
    "def build_denoising_unet(noisy, mu_matrix, p=0.7, is_realnoisy=False, a=0.1):\n",
    "    _, h, w, c = np.shape(noisy)\n",
    "    noisy_tensor = tf.identity(noisy)\n",
    "    is_flip_lr = tf.placeholder(tf.int16)\n",
    "    is_flip_ud = tf.placeholder(tf.int16)\n",
    "    noisy_tensor = data_arg(noisy_tensor, is_flip_lr, is_flip_ud)\n",
    "    response = tf.transpose(noisy_tensor, [0, 3, 1, 2])\n",
    "    mask_tensor = tf.ones_like(response) * 0.7  # Assuming dropout applies here for simplicity\n",
    "\n",
    "    response = tf.multiply(mask_tensor, response)\n",
    "    if is_realnoisy:\n",
    "        response = tf.squeeze(tf.random_poisson(25 * response, [1]) / 25, 0)\n",
    "\n",
    "    response = partial_conv_unet(response, mask_tensor, channel=c, width=w, height=h, p=p)\n",
    "    response = tf.transpose(response, [0, 2, 3, 1])\n",
    "    mask_tensor = tf.transpose(mask_tensor, [0, 2, 3, 1])\n",
    "\n",
    "    response = data_arg(response, is_flip_lr, is_flip_ud)\n",
    "\n",
    "    # Compute the conditional probabilities as an additional loss term\n",
    "    conditional_prob = tf.reduce_mean(compute_conditional_probabilities(response, mask_tensor, mu_matrix))\n",
    "    print(conditional_prob)\n",
    "    original_loss = mask_loss(response, noisy_tensor, 1. - mask_tensor)\n",
    "    data_loss = original_loss - a * conditional_prob\n",
    "    data_loss = tf.maximum(data_loss, 0)  # Ensuring loss doesn't go below 0\n",
    "    print(\"Data loss shape:\", data_loss.get_shape())\n",
    "\n",
    "    # Maintain a running average of the output images\n",
    "    slice_avg = tf.get_variable('slice_avg', shape=[_, h, w, c], initializer=tf.initializers.zeros())\n",
    "    avg_op = slice_avg.assign(slice_avg * 0.99 + response * 0.01)\n",
    "\n",
    "    training_error = data_loss\n",
    "    tf.summary.scalar('data loss', data_loss)\n",
    "\n",
    "input_dir = 'data/train/output'\n",
    "mu_dir = 'data/train/output_corr'\n",
    "output_probabilities = []\n",
    "probs = []\n",
    "\n",
    "\n",
    "for x in range(10):  # the 10th image\n",
    "    image_path = os.path.join(input_dir, f'train_speckle_{x}.png')\n",
    "    mu_path = os.path.join(mu_dir, f'complex_train_speckle_{x}.npy')\n",
    "    mu_matrix = np.load(mu_path)\n",
    "    noisy = util.load_np_image(file_path)\n",
    "    conditional_prob =  build_denoising_unet(noisy, mu_matrix, p=0.7, is_realnoisy=False, a=0.1)\n",
    "    output_probabilities.append(conditional_prob)\n",
    "\n",
    "for idx, avg_prob in enumerate(output_probabilities):\n",
    "    print(f\"Average valid conditional probabilities for image {idx}: {avg_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-31T21:36:20.874229Z",
     "iopub.status.busy": "2024-07-31T21:36:20.873872Z",
     "iopub.status.idle": "2024-07-31T21:36:21.745824Z",
     "shell.execute_reply": "2024-07-31T21:36:21.745172Z",
     "shell.execute_reply.started": "2024-07-31T21:36:20.874208Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import maximum_filter\n",
    "\n",
    "def max_pool_and_expand(image):\n",
    "    # max pooling in 2*2\n",
    "    pooled = maximum_filter(image, size=2, footprint=np.ones((2, 2)), mode='reflect')\n",
    "    # pick only the top-left pixel\n",
    "    pooled = pooled[::2, ::2]\n",
    "    # expand to 2*2 blocks by kronecker\n",
    "    expanded_image = np.kron(pooled, np.ones((2, 2)))\n",
    "    return expanded_image\n",
    "\n",
    "pooled_train_images = [max_pool_and_expand(img) for img in processed_train_images]\n",
    "pooled_val_images = [max_pool_and_expand(img) for img in processed_val_images]\n",
    "\n",
    "plt.imshow(pooled_train_images[0], cmap='gray')\n",
    "plt.title('Pooled Training Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(pooled_val_images[0], cmap='gray')\n",
    "plt.title('Pooled Validation Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:03:11.754953Z",
     "iopub.status.busy": "2024-07-31T22:03:11.754467Z",
     "iopub.status.idle": "2024-07-31T22:03:11.889876Z",
     "shell.execute_reply": "2024-07-31T22:03:11.889328Z",
     "shell.execute_reply.started": "2024-07-31T22:03:11.754924Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Nslm = 16384  # nb of SLM pixels\n",
    "Nfluo = 16384   # nb of beads\n",
    "Ncam = 16384  # nb of camera pixels\n",
    "\n",
    "pooled_train_images_flat = [img.reshape(-1) for img in pooled_train_images] \n",
    "pooled_val_images_flat = [img.reshape(-1) for img in pooled_val_images]\n",
    "\n",
    "SLM_input_train_pooled = np.stack(pooled_train_images_flat, axis=1)\n",
    "SLM_input_val_pooled = np.stack(pooled_val_images_flat, axis=1)\n",
    "\n",
    "SLM_input_train_pha_pooled = torch.from_numpy(SLM_input_train_pooled).float().to(device)\n",
    "SLM_input_train_t_pooled = phase_to_complex(SLM_input_train_pha_pooled)\n",
    "\n",
    "SLM_input_val_pha_pooled = torch.from_numpy(SLM_input_val_pooled).float().to(device)\n",
    "SLM_input_val_t_pooled = phase_to_complex(SLM_input_val_pha_pooled)\n",
    "\n",
    "field1_train_t_pooled = complex_matmul(T1_t, SLM_input_train_t_pooled)\n",
    "int1_train_t_pooled = torch.sum(torch.abs(field1_train_t_pooled)**2,-1)\n",
    "\n",
    "field1_val_t_pooled = complex_matmul(T1_t, SLM_input_val_t_pooled)\n",
    "int1_val_t_pooled = torch.sum(torch.abs(field1_val_t_pooled)**2,-1)\n",
    "# int1_val_t = int1_val_t.T\n",
    "# CAM_output_t = T2_t@int1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:05:54.125821Z",
     "iopub.status.busy": "2024-07-31T22:05:54.125197Z",
     "iopub.status.idle": "2024-07-31T22:05:54.277380Z",
     "shell.execute_reply": "2024-07-31T22:05:54.276792Z",
     "shell.execute_reply.started": "2024-07-31T22:05:54.125800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "np.save('down-sampled/train/train_image.npy', SLM_input_train_pooled)\n",
    "\n",
    "torch.save(int1_train_t_pooled.cpu(), 'down-sampled/train/train_speckle.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:05:55.398602Z",
     "iopub.status.busy": "2024-07-31T22:05:55.398273Z",
     "iopub.status.idle": "2024-07-31T22:05:55.439989Z",
     "shell.execute_reply": "2024-07-31T22:05:55.439457Z",
     "shell.execute_reply.started": "2024-07-31T22:05:55.398583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('down-sampled/val/val_image.npy', SLM_input_val_pooled)\n",
    "\n",
    "torch.save(int1_val_t_pooled.cpu(), 'down-sampled/val/val_speckle.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:08:46.732372Z",
     "iopub.status.busy": "2024-07-31T22:08:46.732039Z",
     "iopub.status.idle": "2024-07-31T22:08:47.358204Z",
     "shell.execute_reply": "2024-07-31T22:08:47.357616Z",
     "shell.execute_reply.started": "2024-07-31T22:08:46.732345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = 'down-sampled/train/input'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, img in enumerate(pooled_train_images):\n",
    "    img = (img - img.min()) / (img.max() - img.min())  \n",
    "    img = (img * 255).astype(np.uint8)  \n",
    "\n",
    "    image = Image.fromarray(img)\n",
    "\n",
    "    file_path = os.path.join(output_dir, f'train_image_{idx}.png')\n",
    "    # saved as .tif\n",
    "    image.save(file_path)\n",
    "\n",
    "print(f\"Saved {len(pooled_train_images)} images to {output_dir}\")\n",
    "\n",
    "file_name = 'train_image_0.png'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray')  # grayscale\n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:08:49.733354Z",
     "iopub.status.busy": "2024-07-31T22:08:49.733007Z",
     "iopub.status.idle": "2024-07-31T22:08:49.940200Z",
     "shell.execute_reply": "2024-07-31T22:08:49.939627Z",
     "shell.execute_reply.started": "2024-07-31T22:08:49.733334Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = 'down-sampled/val/input'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, img in enumerate(pooled_val_images):\n",
    "    img = (img - img.min()) / (img.max() - img.min()) \n",
    "    img = (img * 255).astype(np.uint8) \n",
    "\n",
    "    image = Image.fromarray(img)\n",
    "    file_path = os.path.join(output_dir, f'val_image_{idx}.png')\n",
    "    image.save(file_path)\n",
    "\n",
    "print(f\"Saved {len(pooled_val_images)} images to {output_dir}\")\n",
    "\n",
    "file_name = 'val_image_0.png'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray') \n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:10:01.886079Z",
     "iopub.status.busy": "2024-07-31T22:10:01.885594Z",
     "iopub.status.idle": "2024-07-31T22:10:08.891877Z",
     "shell.execute_reply": "2024-07-31T22:10:08.891353Z",
     "shell.execute_reply.started": "2024-07-31T22:10:01.886057Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "speckle_data = torch.load('down-sampled/train/train_speckle.npy')\n",
    "\n",
    "output_dir = 'down-sampled/train/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(speckle_data.shape[1]): \n",
    "    image = speckle_data[:, i].reshape(128, 128).numpy() \n",
    "    \n",
    "    image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    image_scaled = (image_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    output_filename = os.path.join(output_dir, f'train_speckle_{i}.png')\n",
    "    \n",
    "    plt.imsave(output_filename, image_scaled, cmap='gray')\n",
    "\n",
    "file_name = 'train_speckle_0.png' \n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray') \n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-31T22:10:55.045493Z",
     "iopub.status.busy": "2024-07-31T22:10:55.045144Z",
     "iopub.status.idle": "2024-07-31T22:10:56.485756Z",
     "shell.execute_reply": "2024-07-31T22:10:56.485250Z",
     "shell.execute_reply.started": "2024-07-31T22:10:55.045473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "speckle_data = torch.load('down-sampled/val/val_speckle.npy')\n",
    "\n",
    "output_dir = 'down-sampled/val/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(speckle_data.shape[1]): \n",
    "    image = speckle_data[:, i].reshape(128, 128).numpy()\n",
    "    \n",
    "    image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    image_scaled = (image_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    output_filename = os.path.join(output_dir, f'val_speckle_{i}.png')\n",
    "    \n",
    "    plt.imsave(output_filename, image_scaled, cmap='gray')\n",
    "\n",
    "file_name = 'val_speckle_0.png'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "image = Image.open(file_path)\n",
    "\n",
    "plt.imshow(image, cmap='gray') \n",
    "plt.title(f'Displaying {file_name}')\n",
    "plt.axis('off')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
